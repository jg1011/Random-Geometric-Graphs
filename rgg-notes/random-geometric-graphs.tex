\documentclass{article} 

\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\usepackage{enumerate}
\usepackage[a4paper, margin=1in]{geometry} 
\usepackage{titling}
\usepackage{amssymb}
\usepackage{lipsum}
\usepackage{mathtools}
\usepackage{amsthm} % For proof environments only, labelling of thms done with custom counter
\usepackage{amsmath}
\usepackage{amssymb}

%%% Misc commands %%%
\newcommand\iidsim{\stackrel{\mathclap{\tiny\mbox{i.i.d}}}{\sim}}
\newcommand\indist{\stackrel{\mathclap{\tiny\mbox{$\mathcal{D}$}}}{\longrightarrow}}
\newcommand\nidist{\stackrel{\mathclap{\tiny\mbox{$\mathcal{D}$}}}{\longleftarrow}}
\newcommand{\indep}{\perp\!\!\!\!\perp} 

\setlength{\parindent}{0pt} % Remove indentation upon new paragraph. 

%%% Title page information %%%
\title{Random Geometric Graphs}
\author{Jacob Green}
\date{\today}
\newcommand{\subtitle}{A collection of notes from my time working under the supervision of Mathew Penrose on 
random geometric graph theory and related topics.}
\newcommand{\institution}{Department of Mathematical Sciences, The University of Bath}
\newcommand{\keywords}{Random geometric graphs, Poisson Point Processes, Combinatorial Geometry, Sphere Packing}

\newcounter{definitioncount} % tcolorbox counter, for ease of self-reference
\newcounter{lemmacount}
\newcounter{examplecount}
\newcounter{theoremcount}
\newcounter{propositioncount}
\newcounter{corollarycount}
\newcounter{remarkcount}
\counterwithin{corollarycount}{section}
\counterwithin{propositioncount}{section}
\counterwithin{remarkcount}{section}
\counterwithin{theoremcount}{section}
\counterwithin{examplecount}{section}
\counterwithin{lemmacount}{section}
\counterwithin{definitioncount}{section}
\renewcommand{\thecorollarycount}{\thesection.\arabic{corollarycount}}
\renewcommand{\thepropositioncount}{\thesection.\arabic{propositioncount}}
\renewcommand{\theremarkcount}{\thesection.\arabic{remarkcount}}
\renewcommand{\thetheoremcount}{\thesection.\arabic{theoremcount}}
\renewcommand{\theexamplecount}{\thesection.\arabic{examplecount}}
\renewcommand{\thelemmacount}{\thesection.\arabic{lemmacount}}
\renewcommand{\thedefinitioncount}{\thesection.\arabic{definitioncount}}
\newcounter{claimcount} % Used for proof with lots of claims

\newtcolorbox{definition}[2][auto counter, number within=section]{
  colback=black!5!white, 
  colframe=black!50!white, 
  fonttitle=\bfseries, 
  coltitle=black,
  title=Definition \thedefinitioncount $\:$ (#2), 
  before upper = \refstepcounter{definitioncount},
  #1, % Optional params
}

\newtcolorbox{lemma}[2][auto counter, number within=section]{
  colback=blue!5!white, 
  colframe=blue!50!white, 
  fonttitle=\bfseries, 
  coltitle=black, 
  title=Lemma \thelemmacount $\:$ (#2),
  before upper = \refstepcounter{lemmacount} 
  #1, % Optional params
}

\newtcolorbox{example}[2][auto counter, number within=section]{
  colback=green!5!white, 
  colframe=green!50!white, 
  fonttitle=\bfseries, 
  coltitle=black, 
  title=Example \theexamplecount $\:$ (#2), 
  before upper = \refstepcounter{examplecount}
  #1, % Optional params
}

\newtcolorbox{theorem}[2][auto counter, number within=section]{
  colback=red!5!white,
  colframe=red!50!white, 
  fonttitle=\bfseries, 
  coltitle=black, 
  title=Theorem \thetheoremcount $\:$ (#2), 
  before upper = \refstepcounter{theoremcount},
  #1, % Optional params
}

\newtcolorbox{proposition}[2][auto counter, number within=section]{
  colback=purple!5!white,
  colframe=purple!50!white, 
  fonttitle=\bfseries, 
  coltitle=black, 
  title=Proposition \thepropositioncount $\:$ (#2), 
  before upper = \refstepcounter{propositioncount},
  #1, % Optional params
}

\newtcolorbox{corollary}[2][auto counter, number within=section]{
  colback=yellow!5!white,
  colframe=yellow!50!white, 
  fonttitle=\bfseries, 
  coltitle=black, 
  title=Corollary \thecorollarycount $\:$ (#2),
  before upper = \refstepcounter{corollarycount}, 
  #1, % Optional params
}

\newtcolorbox{remark}[2][auto counter, number within=section]{
  colback=black!5!white,
  colframe=black!50!white, 
  fonttitle=\bfseries, 
  coltitle=black, 
  title=Remark \theremarkcount $\:$ (#2),
  before upper = \refstepcounter{remarkcount}, 
  #1, % Optional params
}

\newtcolorbox[use counter=claimcount]{claim}[1][]{%
    colback=green!10, 
    colframe=green!50!black, 
    coltitle=black, 
    fonttitle=\bfseries, 
    boxrule=0.5mm, 
    colbacktitle=green!30,
    enhanced, % Allows additional options
    boxed title style={sharp corners}, 
    attach title to upper={},
    titlerule=0mm, 
    title=Claim: $\;$,
    before=\par\smallskip\noindent, 
    #1
}

\begin{document}

\begin{titlepage}
    \centering
    % Adding an image (optional)
    
    % Title
    {\Huge \bfseries \thetitle \par}
    \vspace{0.5cm}
    
    % Subtitle (if any)
    {\Large \subtitle \par}
    \vspace{1cm}
    
    % Author and institution
    {\large \theauthor \par}
    {\institution \par}
    \vspace{1cm}
    
    % Date
    {\large \thedate \par}
    \vspace{1.5cm}
    
    % Abstract section
    \begin{abstract}
        \lipsum[10]
    \end{abstract}
    \vspace{1cm}
    
    % Keywords section
    \textbf{Keywords:} \keywords
    \vfill % Pushes the following content to the bottom
    
    % Footer or further information
    \textit{}
\end{titlepage}

\newpage

\tableofcontents 

\newpage 

\setcounter{page}{1} % Start page numbering from 1 after title page

\section{Introduction}

% Count over naturals
\setcounter{lemmacount}{1}
\setcounter{examplecount}{1}
\setcounter{theoremcount}{1}
\setcounter{propositioncount}{1}
\setcounter{corollarycount}{1}
\setcounter{remarkcount}{1}
\setcounter{definitioncount}{1}

\subsection{What is a random geometric graph?}

\begin{definition}[]{geometric graph}
    Let $\mathcal{X} \subset \mathbb{R}^d$ be a finite collection of points and $r > 0$. The {\it geometric graph}
    $G(\mathcal{X}, r)$ is the graph with vertex set $\mathcal{X}$ and edge set $\{\{x, y\} \subset \mathcal{X} 
    \; : \; \lvert x - y \rvert \leq r\}$, where $\lvert \cdot \rvert$ denotes the Euclidean norm. 
\end{definition}

We can turn this into a random graph by letting the set $\mathcal{X} \subset \mathbb{R}^d$ be random. The resulting
structure $G(\mathcal{X}, r)$ is said to be the {\it random geometric graph} (A.K.A the {\it Gilbert graph}). The first 
structure we'll consider is found by uniformly scattering $n$ points in the $d$-dimensional hypercube $[0,1]^d$. 

\begin{example}[]{fixed scale geometric graph}
    Let $\xi_1, \xi_2, \dots \iidsim \text{Unif}[0,1]^d$ and fix a sequence $(r_n)_{n \geq 1}$ in $\mathbb{R}_{>0}$.
    Let $\mathcal{X}_n := \{\xi_1, \dots, \xi_n\}$. Then $G(\mathcal{X}_n, r_n)$ is the {\it fixed scale geometric 
    graph} at time $n \geq 1$.
\end{example}

We will also consider what happens when the number of vertices is Poisson.

\begin{example}[]{Poisson scale geometric graph}
    Let $\xi_1, \xi_2, \dots \iidsim \text{Unif}[0,1]^d$ and $N_n \sim \text{Poisson}(n)$ for $n \geq 1$ be independent 
    of $(\xi_1, \xi_2, \dots)$. Let $\mathcal{P}_n := \{\xi_1, \dots, \xi_{N_n}\}$ and fix a sequence $(r_n)_{n \geq 1}$
    in $\mathbb{R}_{> 0}$. Then the random geometric graph $G(\mathcal{P}_n, r_n)$ is the $d$-dimensional {\it Poisson 
    scale geometric graph} at time $n \geq 1$. 
\end{example}

As one may expect, the vertices of this graph form a Poisson point process in $\mathbb{R}^d$. This was originally 
an exercise (1.1) in \cite{Penrose_et_al_2016}.

\begin{proposition}[]{$\mathcal{P}_n(\cdot)$ is a Poisson point process}
    Let $\mathcal{P}_n$ be the vertex set in the Poisson scale geometric graph. Then $\mathcal{P}_n(\cdot): A \mapsto 
    |\mathcal{P}_n \cap A|$, where $A \subset [0,1]^d$ is measurable and $|\cdot|$ is the counting measure, is a 
    {\it Poisson point process} with intensity $\lambda(A)$, $\lambda(\cdot)$ the Lebesgue measure on $\mathbb{R}^n$.
\end{proposition}

See Appendix A for the definition of the Poisson point process. 

\begin{proof}
Write $\mathcal{P}_n(A_j) = \sum_{i=1}^{N_n} {\bf 1}\{\xi_i \in A_j\}$. Then, conditioning on $\{N_n = m\}$, each 
$\mathcal{P}_n(A_j)$ is a $\text{Binom}(m, \lambda(A_j))$ random variable, making the joint distribution (also 
conditioned on $\{N_n = m\}$) a $\text{Multinom}(m, \lambda(A_1), \dots, \lambda(A_k))$. Thus one has 
\begin{align*}\mathbb{P}(\mathcal{P}_n(A_1) = a_1, \dots, \mathcal{P}_n(A_k) = a_k) &= 
\underbrace{\left(\frac{e^{-n}n^m}{m!}\right)}_{\text{Poisson}(n ; m)} \quad \times 
\underbrace{\binom{m}{j_1, \dots, j_k}\prod_{i=1}^k {(\lambda(A_i))}^{j_i}}_{\text{Multinom}(m, \lambda(A_1), \dots, \lambda(A_k) ; j_1, \dots, j_k)} \\
&= \prod_{i=1}^k \frac{e^{-n\lambda(A_i)}(n\lambda(A_i))^{j_i}}{(j_i)!}
\end{align*}
which is a product of the distributions $\text{Poisson}(n\lambda(A_i)), 1 \leq i \leq k$, giving us both of our 
defining properties of a Poisson point process. 
\end{proof}

The exercise (1.2) in \cite{Penrose_et_al_2016} states the following result, which we'll prove in an analogous way.

\begin{proposition}[]{expected degree of $k^\text{th}$ vertex in fixed scale RGG}
    Consider the fixed scale random geometric graph $G(\mathcal{X}_n, r_n)$ with $r_n \to 0$ as $n \to \infty$. Let 
    $D_{k, n}$ be the degree of the first vertex $\xi_k$, then $\mathbb{E}[D_{k, n}] \sim \theta n r_n^d$.
\end{proposition}

\begin{proof}
Write $D_{k,n} = \sum_{i \neq k} {\bf 1}\{\lvert \xi_k - \xi_i \rvert \leq r\}$, then we have $D_{k,n} \sim 
\text{Binom}(n-1, p)$ with \[p = \lambda(B_{r_n}(\xi_k) \cap [0,1]^d)\] where $B_{r_n}(\xi_k)$ is the hypersphere 
radius $r_n$ centred at $\xi_k$ and $\lambda$ the Lebesgue measure on $\mathbb{R}^d$. Now, conditioning on $\xi_k$, 
we have \[\mathbb{E}[D_{k,n} | \xi_k] = (n-1)\lambda(B_{r_n}(\xi_k) \cap [0,1]^d)\]
and hence, by the total law of expectation, 
\begin{align*}
    \mathbb{E}[D_{k,n}] &= \mathbb{E}[\mathbb{E}[D_{k,n} | \xi_k]] \\
    &= (n-1)\int_{[0,1]^d}\lambda(B_{r_n}(\xi_k) \cap [0,1]^d)\mathrm{d}\xi_k \\
    &\sim n\int_{[0,1]^d}\lambda(B_{r_n}(\xi_k))\mathrm{d}\xi_k \sim n(\theta r_n^d)
\end{align*}
Where the first asymptotic equality follows via the dominated convergence theorem (to see this, observe $B_{r_n}(\xi_k) \cap [0,1]^d 
\to B_{r_n}(\xi_k)$ A.E. and $B_{r_n}(\xi_k)$ also dominating). Note $\theta$ is a constant depending on $d$, 
and is the coefficient of $r_n^d$ in the formula for the volume of a hypersphere radius $r_n$.
\end{proof}

\subsection{Counting Edges of $G(\mathcal{X}_n, r_n)$ and $G(\mathcal{P}_n, r_n)$}

The number of edges $\mathcal{E}_n, \mathcal{E}_n^\prime$ in $G(\mathcal{X}_n, r_n)$ and $G(\mathcal{P}_n, r_n)$ respectively 
is a Poisson random variable. To prove this, we'll use a so called {\it dependency graph}. We handle the two 
RGGs $G(\mathcal{X}_n, r_n)$ and $G(\mathcal{P}_n, r_n)$ seperately. 

\subsubsection*{Counting edges of $G(\mathcal{X}_n, r_n)$}

An elementary question to ask is "what is the expected edge count at time $n$?". Via the previously derived 
vertex degree asymptotics, we have an easy asymptotic result.  

\begin{proposition}[]{expected edge count in fixed scale RGG}
    Let $\mathcal{E}_n$ and $\mathcal{E}^\prime_n$ be the number of edges in $G(\mathcal{X}_n, r_n)$ and 
    $G(\mathcal{P}_n, r_n)$ respectively. Then,
    \[\mathbb{E}[\mathcal{E}_n] \sim \theta n^2 r_n^d / 2\]
\end{proposition}

\begin{proof}[Proof of lemma]
    By the handshaking lemma we have $2\mathbb{E}[\mathcal{E}_n] = \sum_{k = 1}^n \mathbb{E}[D_{k, n}] \sim n^2 r_n^d$. 
\end{proof}

Now we introduce the {\it dependency graph}. 

\begin{definition}[]{dependency graph}
    Let $(V, \sim)$ be a finite simple graph w/ edge relation $\sim$ and vertex set $V$. We say $(V, \sim)$ is a 
    {\it dependency graph} for the random variables $(W_\alpha)_{\alpha \in V}$ if whenever $A, B \subset V$ are 
    disjoint with no $\alpha \in A$, $\beta \in B$ such that $\alpha \sim \beta$ (i.e. $A$ and $B$ lie in unique 
    connected components) then \[(W_\alpha)_{\alpha \in A} \indep (W_\beta)_{\beta \in B} \quad \text{i.e. are independent families}\]
\end{definition}

When we take the random variable associated with each vertex to be 0-1, we obtain the following bound. 

\begin{lemma}[]{Poisson approximation lemma for Bernoulli sums}
    Let $(\xi_i)_{i \in I}$ be a finite collection of Bernoulli random variables with dependency graph $(I, \sim)$. 
    Set $p_i := \mathbb{P}(\xi_i = 1)$, $p_{ij} := \mathbb{P}(\xi_i = 1 ; \xi_j = 1)$ and suppose $\lambda := 
    \sum_{i \in I}p_i$ is finite. Then, letting $W := \sum_{i \in I}\xi_i$, we have (LHS is just total variation distance)
    \[\sum_{k \geq 0} \bigg|\mathbb{P}[W = k] - \mathbb{P}(\text{Po}(\lambda) = k)\bigg| \leq 
    \min \{6, 2\lambda^{-1}\} \left(\sum_{i \in I}\sum_{\mathcal{N}(i) \setminus \{i\}}p_{ij} + 
    \sum_{i \in I}\sum_{j \in \mathcal{N}(i)}p_ip_j\right)\]
\end{lemma}

\begin{proof}
    Omitted, consult \cite{Penrose_2003}.
\end{proof}

\begin{theorem}[]{total variation between edge and Poisson distribution}
    Let $\lambda_n := \mathbb{E}[\mathcal{E}_n]$. Then 
    \[\sum_{k \geq 0} | \mathbb{P}(\mathcal{E}_n = k) - \mathbb{P}(\text{Po}(\lambda_n) = k) | = O(nr^d)\]
\end{theorem}

\setcounter{claimcount}{1} % Using claims, reset the count

The idea of this proof is to write $\mathcal{E}_n = \sum_{1 \leq i < j \leq n} {\bf 1}\{\lvert \xi_i - \xi_j \rvert \leq r_n\}$,
i.e. the number of edges as the sum of the indicators over all possible edges indicating whether this edge 
exists. This has a rather obvious dependency graph and hence we can apply the Poisson approximation lemma (1.7)
with some work on the asymptotics.

\begin{proof}
Let $V \{\{i, j\} : 1 \leq i < j \leq\}$ and define $\sim$ by $\alpha \sim \beta$ if $\alpha \cap \beta \neq \emptyset$ 
and $\alpha \neq \beta$. Define the random variables $W_\alpha = {\bf 1}\{\lvert \xi_i - \xi_j \rvert \leq r_n\}$ for 
$\alpha \in V$ and let $\lambda_n := \sum_{\alpha \in V}p_\alpha$.

\begin{claim}[]{}
    $G(V, \sim)$ is a dependency graph for $(W_\alpha)_{\alpha \in V}$.
\end{claim}

\begin{proof}[Proof of claim]
    Follows immediately from the independence of the $\xi_i$.
\end{proof}

Let $p_\alpha := \mathbb{P}(W_\alpha = 1)$ and $p_{\alpha \beta} := \mathbb{P}(W_\alpha = 1 ; W_\beta = 1)$, as in 
the setup of lemma 1.7.

\begin{claim}[]{}
    $p_\alpha \sim \theta r_n^d$
\end{claim}

\begin{proof}[Proof of claim]
    Fix $\alpha = \{i, j\} \in V$. Then, letting $\lambda(\cdot)$ be the Lebesgue measure on $\mathbb{R}^d$, 
    \begin{align*}
        p_\alpha = \mathbb{P}(\lvert \xi_i - \xi_j \rvert \leq r_n) 
        &= \int_{v \in [0,1]^d} \mathbb{P}(\lvert \xi_i - \xi_j \rvert \leq r_n | \xi_i = v)\mathrm{d}v \\ 
        &= \int_{v \in [0,1]^d} \lambda(B_{r_n}(v) \cap [0,1]^d) \mathrm{d}v \\
        &\sim \int_{v \in [0,1]^d} \lambda(B_{r_n}(v))\mathrm{d}v \sim \theta r_n^d
    \end{align*}
    where the first asymptotic equality follows from the dominated convergence theorem.
\end{proof}

\begin{claim}[]{}
    $p_{\alpha \beta} \sim (\theta r_n^d)^2$
\end{claim}

\begin{proof}[Proof of claim]
    An analogous argument works. 
\end{proof}

Now we have all our ingredients, lets cook. Observe, from the previously computed asymptotics, 
\[\lambda_n \sim n^2\theta r_n^d/2 \quad \text{and} \quad \sum_{\alpha \in V}p_\alpha^2 \sim \theta \lambda_n r_n^d\]
and hence by counting $|\mathcal{N}(\alpha)| = 2(n-2)$, obtain 
\[\sum_{\alpha \in V}\sum_{\beta \sim \alpha}(p_{\alpha \beta} + p_\alpha p_\beta) \sim 
\binom{n}{2} \times 2(n-2) \times 2(\theta r_n^d)^2 = O(n\lambda_n r_n^d)\]
which by the Poisson approximation lemma gives
\[\sum_{k \geq 0}|\mathbb{P}(\mathcal{E}_n = k) - e^{-\lambda_n}\lambda_n^k/k!| = O(nr_n^d)\]  
as claimed. 
\end{proof}

\begin{corollary}[]{edge distribution of the fixed scale random geometric graph}
    If the limit $\lambda_n \to \lambda \in (0,\infty)$ exists and $nr_n^d \to 0$, then 
    $\mathcal{E}_n \indist \text{Poisson}(\lambda)$
\end{corollary}

\begin{proof}
    By proposition 1.6, we have $\lambda \sim \lambda_n \sim \theta n^2 r_n^d/2 \Rightarrow n^2 r_n^d \sim 2\lambda/\theta 
    \in (0, \infty)$ which forces $n r_n^d \to 0$ and hence lemma 1.8 applies giving $\mathcal{E}_n \indist \text{Poisson}(\lambda)$.
\end{proof}

\subsubsection*{Counting edges of $G(\mathcal{P}_n, r_n)$}

We now spend the rest of the (sub)section proving similar results for $G(\mathcal{P}_n, r_n)$. Before proceeding we have
the following lemma from \cite{Penrose_et_al_2016}. We write $\mathcal{P}_{< \infty}(A)$ for the family of finite subsets of $A$. 

\begin{lemma}[]{Mecke formula}
    Let $k \in \mathbb{N}$. For any measurable $f: (\mathbb{R}^d)^k \times \mathcal{P}_{< \infty}([0,1]^d) \to \mathbb{R}$, 
    when the expectation exists, 
    \[\mathbb{E}\sum_{X_1, \dots, X_k \in \mathcal{P}_n}^{\neq} f(X_1, \dots, X_k, \mathcal{P}_n \setminus \{X_1, \dots, X_n\})
    = n^k \int \dots \int \mathbb{E}f(x_1, \dots, x_k, \mathcal{P}_n)\mathrm{d}x_1 \dots \mathrm{d}x_k\]
    where $\sum^{\neq}$ denotes the sum over the ordered $k$-tuples of distinct points (in $\mathcal{P}_n$).
\end{lemma}

\begin{proof}
The idea is to condition on the number of points in $\mathcal{P}_n$. Observe, 
\begin{align*}
    \mathbb{E}&\sum_{X_1, \dots, X_k \in \mathcal{P}_n}^{\neq} f(X_1, \dots, X_k, \mathcal{P}_n \setminus \{X_1, \dots, X_n\}) \\
    &= \mathbb{E}\left[\mathbb{E}\left[\sum_{X_1, \dots, X_k \in \mathcal{P}_n}^{\neq} f(X_1, \dots, X_k, \mathcal{P}_n \setminus \{X_1, \dots, X_n\}) \bigg| |\mathcal{P}_n| = m\right]\right] \\
    &= \sum_{m \geq k}\left(\frac{e^{-n}n^m}{m!}\right)\sum_{X_1, \dots, X_k \in \{\xi_1, \dots, \xi_m\}}^{\neq}\mathbb{E}f(X_1, \dots, X_k, \{X_{k+1}, \dots, X_{m}\}) \\
    (\dagger) \qquad \qquad &= \sum_{m \geq k}\left(\frac{e^{-n}n^m}{m!}\right)\left(\prod_{i = 1}^{k}(m-i+1)\right)\mathbb{E}f(X_1, \dots, X_k, \{X_{k+1}, \dots, X_{m}\}) \\ 
    &= \sum_{m \geq k}\left(\frac{e^{-n}n^m}{m!}\right)\left(\prod_{i = 1}^{k}(m-i+1)\right)\int_{[0,1]^d} \dots \int_{[0,1]^d} f(x_1, \dots, x_k, \{x_{k+1}, \dots, x_m\})\mathrm{d}x_1 \dots \mathrm{d}x_m \\
    &= n^k \sum_{m \geq k}\left(\frac{e^{-n}n^{m-k}}{(m-k)!}\right) \int_{[0,1]^d} \dots \int_{[0,1]^d} f(x_1, \dots, x_k, \{x_{k+1}, \dots, x_m\})\mathrm{d}x_1 \dots \mathrm{d}x_m \\
    &= n^k \int_{[0,1]^d} \dots \int_{[0,1]^d} f(x_1, \dots, x_k, \{x_{k+1}, \dots, x_m\})\mathrm{d}x_1 \dots \mathrm{d}x_m  \\
    &= n^k \int_{[0,1]^d} \dots \int_{[0,1]^d} \mathbb{E}f(x_1, \dots, x_k, \mathcal{P}_n)\mathrm{d}x_1 \dots \mathrm{d}x_k
\end{align*}
Where $(\dagger)$ follows from the $\xi_i$ being i.i.d, so all expectations are equal and it suffices to count 
distinct $k$-tuples in $\{\xi_1, \dots, \xi_m\}$. [CHECK THIS PROOF W/ PENROSE]
\end{proof}

We can now use this lemma to find the expected number of edges in $G(\mathcal{P}_n, r_n)$. 

\begin{proposition}[]{expected edge count in Poisson scale RGG}
    Let $G(\mathcal{P}_n, r_n)$ be the Poisson scale RGG and $\mathcal{E}_n$ be the number of edges at time $n$. Then, 
    \[\mathbb{E}[\mathcal{E}_n] \sim \theta n^2 r_n^d / 2\]
\end{proposition}

\begin{proof}
Write $\mathcal{E}_n = \frac{1}{2}\sum_{X_1, X_2 \in \mathcal{P}_n}^{\neq}{\bf 1}\{\lVert X_1 - X_2 \rVert \leq r_n\}$. Then 
we can use Mecke's formula as follows. 
\begin{align*}
    \mathbb{E}[\mathcal{E}_n] &= \frac{1}{2}\mathbb{E}\left[\sum_{X_1, X_2 \in \mathcal{P}_n}^{\neq}{\bf 1}\{\lVert X_1 - X_2 \rVert \leq r_n\}\right] \\
    (\text{Mecke}) \quad &= \frac{n^2}{2}\int_{[0,1]^d}\int_{[0,1]^d}\mathbb{E}[{\bf 1}\{\lVert x_1 - x_2 \rVert \leq r_n\}]\mathrm{d}x_1\mathrm{d}x_2 \\
    &= \frac{n^2}{2}\int_{[0,1]^d}\int_{[0,1]^d}{\bf 1}\{\lVert x_1 - x_2 \rVert \leq r_n\}\mathrm{d}x_1\mathrm{d}x_2 \\
    &= \frac{n^2}{2}\int_{[0,1]^d}\lambda(B(x_1, r_n) \cap [0,1]^d)\mathrm{d}x_1 \sim \theta n^2 r_n^d / 2
\end{align*}
where the final asymptotic equality follows from the dominated convergence theorem and volume of a $d$-dimensional 
hypersphere. Note we have already seen the asymptotics for the final integral.
\end{proof}

Rest follows on from Exercise (2.2), too small brained to figure it out right now will attempt again tomorrow $:<$

\subsection{A central limit theorem for our edge counts}

\begin{lemma}[]{Falling factorial moment for the Poisson distribution}
    Let $X \sim \text{Poisson}(\lambda)$ and fix $k \in \mathbb{N}$. Then $\mathbb{E}[X(X-1)\dots (X-k+1)] = \lambda^k$
\end{lemma}

\begin{proof}
    This follows by the following simple calculation.
    \begin{align*}
        \mathbb{E}[&X(X-1)\cdots (X - k + 1)] = \sum_{x \geq 0}x(x-1)\cdots (x-k+1)\frac{e^{-\lambda}\lambda^x}{x!} \\ 
        &= e^{-\lambda}\sum_{x \geq k}x(x-1)\cdots (x-k+1)\frac{\lambda^x}{x!} = \lambda^ke^{-\lambda}\sum_{x \geq k}
        \frac{\lambda^{x-k}}{(x-k)!} = \lambda^k
    \end{align*}
\end{proof}

\newpage

\section{Sphere packing with random geometric graphs}

% Count over naturals
\setcounter{lemmacount}{1}
\setcounter{examplecount}{1}
\setcounter{theoremcount}{1}
\setcounter{propositioncount}{1}
\setcounter{corollarycount}{1}
\setcounter{remarkcount}{1}
\setcounter{definitioncount}{1}

We denote the maximal density unit-volume sphere packing in $\mathbb{R}^d$ by $\theta(d)$. Formally, we call 
a collection of points $\mathcal{P} \subset \mathbb{R}^d$ a {\it packing} if $\lVert x - y \rVert_2 \geq 2r_d$ for 
all $x,y \in \mathcal{P}$, where $r_d$ is the radius of the unit-volume $d$-dimensional hypersphere. Letting 
${\bf \mathcal{P}}$ be the collection of all packings in $\mathbb{R}^d$, we may define 
\[\theta(d) := \sup_{\mathcal{P} \in {\bf \mathcal{P}}} \limsup_{R \to \infty}
\frac{|\mathcal{P} \cap B_{{\bf 0}}(R)|}{\text{Vol}(B_{{\bf 0}}(R))}\]
where $\text{Vol}(\cdot)$ is the Lebesgue measure in $\mathbb{R}^d$ and $B_0(R)$ is the ball radius $R$ centred at 
${\bf 0} \in \mathbb{R}^d$. 

\subsection{A new bound for sphere packing in $\mathbb{R}^d$ as $d \to \infty$}

In this note, we follow the work of Campos $\&$ Co to prove $\theta(d) \to (1 - o(1))\frac{d\log d}{2^{d+1}}$ as 
$d \to \infty$. Campos $\&$ Co utilis a Poisson random geometric graph to improve Hayes' 1947 bound on the maximal density 
of a sphere packing in high dimensional $\mathbb{R}^d$ by a factor of $\log d$. It is worth noting that this is still 
an exponential factor behind the best upper bound, which currently is $2^{(-0.599\cdots + o(1))d}$. The explicit 
value of $\theta(d)$ is only known for $d \in \{1,2,3,8,24\}$. 

\begin{theorem}[]{$\theta(d)$ bound / theorem 1.1 in \cite{campos2023}}
    As $d \to \infty$, one has \[\theta(d) \geq (1 - o(1))\frac{d\log d}{2^{d+1}}\] 
\end{theorem}

Of particular interest is the structural irregularity of the packings used to prove this theorem. Previous work 
relied on lattice packings, whereas this work constructs packings by recursively adding random spheres to $\mathbb{R}^d$
that are centred on points of an appropriately modified Poisson process. \\

Campos $\&$ Co also managed to adapt their techniques to the problem of spherical codes. I will elect to leave that 
to their paper (see Theorem 1.2 $\&$ Appendix A. in \cite{campos2023}).

\subsubsection*{Proof of $\theta(d)$ bound}

Here I describe the method. 
\begin{enumerate}[(i)]
    \item Discretize space with a Poisson process at a carefully chosen intensity.
    \item Impose additional uniformity properties on the discrete point set given in (i), giving $X \subset \mathbb{R}^d$.
    \item Consider geometric graph $G = G(X, 2r_d)$, $r_d$ the radius of the $d$-dimensional unit-volume ball.
    \item Bound $\alpha(G)$, the largest independent set in $G$. 
\end{enumerate}

In regards to step (iv) we have the following new tool, certainly of independent interest. Let $\Delta(G)$ be the 
maximum vertex degree in a graph $G$ and let $\Delta_\text{co}(G)$ denote the maximum codegree (that is, the maximal 
size of $|\mathcal{N}(u)\cap \mathcal{N}(v)|$ over vertices $u, v \in V$). 

\begin{theorem}[]{$\alpha(G)$ bound / theorem 1.3 in \cite{campos2023}}
    Let $G$ be a graph on $n$ vertices with $\Delta(G) \leq \Delta$ and $\Delta_\text{co}(G) \leq C\Delta (\log \Delta)^{-c}$. 
    Then, \[\alpha(G) \geq (1 - o(1))\frac{n \log \Delta}{\Delta}\] where $o(1) \to 0$ as $\Delta \to \infty$.
\end{theorem}

In fact, one can take $C = 2^{-7}$ and $c = 7$. \\

This bound is sharp up to some constants $c$ and $C$. To see this, $\dots$ [Not sure I fully get this, pls fix]. \\

With this tool, the proof of our $\theta(d)$ bound is remarkably easy. We just need the following lemma. 

\begin{lemma}[]{large uniform geometric graphs / lemma 2.1 in \cite{campos2023}}
    Let $\Omega \subset \mathbb{R}^d$ be bounded and measurable. Then $\forall d \geq 1000 \; \exists X \subset \Omega$ 
    with \[|X| \geq (1-1/d)\frac{\Delta}{2^d}\text{Vol}(\Omega), \quad \text{where} \quad \Delta = 
    \left(\frac{\sqrt{d}}{4\log d}\right)^d\] and, setting $G = G(X, r_d)$ we have 
    \[\Delta(G) \leq \Delta(1 + \Delta^{-1/3}) \quad \text{and} \quad \Delta_\text{co} \leq \Delta e^{-(\log d)^2/8}\]
\end{lemma}

\begin{proof}[Proof of $\theta(d)$ bound / Theorem 1.1 in \cite{campos2023}]
    Fix $R > 0$ and set $\Omega = B_{{\bf 0}}(R)$. It suffices to prove that we can place \[(1 - o(1))\text{Vol}(\Omega)
    \frac{d\log d}{2^{d+1}}\] points each at pairwise distance at least $2r^d$. The previous lemma 
    gives us $X \subset \Omega$ with \[|X| \geq (1-o(1))\text{Vol}(\Omega)\Delta 2^{-d}\] with $G(X, 2r_d)$ satisfying 
    the conditions of our $\alpha(G)$ bound. Applying this gives an independent set $I$ with \[|I| \geq (1-o(1))
    \frac{|X|\log \Delta}{\Delta} \geq (1-o(1))\text{Vol}(\Omega)\frac{d\log d}{2^{d+1}}\] and we're done.
\end{proof}

To prove lemma 2.1 we'll consider a Poisson point process with intensity $\lambda=2^{-d}\Delta=
\left(\frac{\sqrt{d}}{8\log d}\right)^d$ and remove "bad" points $x \in X$ with 
\[|X \cap B_x(2r_d)| \geq \Delta(1+\Delta^{-1/3}) \quad \text{or} \quad |X \cap B_x(2_r) \cap B_y(2r_d)| \geq 
\Delta e^{-(\log d)^2/8} \tag{$\heartsuit$}\] for some $y \in X$. These bad points are of course those that have too high a degree 
or codegree to let us be in the conditions of our $\alpha(G)$ bound for $G = G(X, 2r_d)$. To prove we don't delete 
too many points, we will make repeated use of the previously seen Mecke equation, stated in the following form. 

\begin{lemma}[]{univariate Mecke}
    Let $\Omega \subset \mathbb{R}^d$ be bounded and measurable, and let $X \sim \text{Po}_\lambda(\Omega)$ be a
    Poisson point process rate $\lambda$ on $\Omega$. Then, for events $(A_x)_{x \in \Omega}$ 
    \[\mathbb{E}[|x \in X \cap \Omega :  A_x \text{ holds on } X|] = \lambda\int_\Omega \mathbb{P}(A_x \text{ holds on } X \cup \{x\})dx\]
\end{lemma}

\begin{proof}
    Just a special case of lemma 1.11.
\end{proof}

We will also make use of the following elementary tail bound for Poisson random variables

\begin{lemma}[]{Poisson tail bound}
    Let $Y \sim \text{Po}(\lambda)$ be a Poisson random variable with rate $\lambda$. Then, for all $t > 0$,
    \[\mathbb{P}(Y - \lambda \geq \lambda t) \leq \exp(-\lambda\min\{t, t^2\}/3)\]
\end{lemma}

\begin{proof}
    Smells like a Chernoff type bound, some napkin mathematics (using chernoff and $1+x \leq e^x$) gives me a candidate 
    RHS of $e^{-\lambda t^2}$. I can't, off the top, of my head beat this though.
\end{proof}

Firstly, we'll prove the number of points with too high a degree is small.

\begin{lemma}[]{number of high degree points is small / lemma 2.2 in \cite{campos2023}}
    Let $\Omega \subset \mathbb{R}^d, d \geq 4$ be a bounded $\&$ measurable subset and 
    $\mathcal{X} \sim \text{Po}_{\lambda}(\Omega)$. Then, 
    \[\mathbb{E}[|\{x \in X : |X \cap B_x(2r_d)| \geq \Delta(1+\Delta^{-1/3})\}|] \leq \frac{1}{2d}\mathbb{E}[|X|], 
    \quad \text{with } \lambda = 2^{-d}\Delta = \left(\frac{\sqrt{d}}{8 \log d}\right)^d\] 
\end{lemma}

\begin{proof}
    Fix $s > 0$. Then Mecke's equation gives us 
    \[\mathbb{E}[|\{x \in \mathcal{X} : |\mathcal{X} \cap B_x(2r_d)| \geq s\}|] 
    = \lambda \int_{\Omega} \mathbb{P}[|\mathcal{X} \cap B_x(2r_d)| \geq s-1] \tag{$\dagger$}\]
    Now, for fixed $x \in \Omega$, $|\mathcal{X} \cap B_x(2r_d)|$ is a Poisson random variable of rate at most 
    $\lambda 2^d =: \Delta$, which follows from noting $\text{Vol}(B_x(2r_d) \cap X) \leq 2^d$. Hence, via our 
    Poisson tail bounds, one has 
    \[\mathbb{P}[|\mathcal{X} \cap B_x(2r_d)| \geq \Delta(t+1)] 
    \leq \exp\left(-\min\{t, t^2\}\Delta/3\right) \quad \text{for } t > 0\] 
    Taking $t = \Delta^{-1/3} - \Delta^{-1}$ and substuting this into $(\dagger)$ gives us the result.
\end{proof}

Now we do the same with the codegree, which will take more work. 

\begin{lemma}[]{number of high codegree points is small}
    Let $\Omega \subset \mathbb{R}^d$ be bounded and measurable, $\mathcal{X} \sim \text{Po}_\lambda(\Omega)$ and set 
    $\eta = e^{-(\log d)^2/8}$. Then, \[\mathbb{E}\left[\left|\{x \in \mathcal{X} : \exists y \in \mathcal{X} \text{ with }
    |\mathcal{X} \cap B_x(2r_d) \cap B_y(2r_d)| \geq \eta \Delta\}\right|\right] \leq \frac{1}{2d}\mathbb{E}[|\mathcal{X}|]\]
\end{lemma}

\begin{proof}
    Set $I_{x,y} = |\mathcal{X} \cap B_x(2r_d) \cap B_y(2r_d)|$. Then applying Meckes, obtain 
    \[\text{LHS} = \mathbb{E}\left|\{x \in \mathcal{X} : \exists y \in \mathcal{X} \text{ with } I_{x,y}
    \geq \eta \Delta\}\right| = \lambda \int_{\Omega}\mathbb{P}(\exists y \in \mathcal{X} : I_{x,y} \geq 
    \eta \Delta -1)\mathrm{d}x\] 
    Thus it suffices to prove that $\mathbb{P}(\exists y \in \mathcal{X} : I_{x,y} \geq \eta \Delta - 1) \leq \frac{1}{2d}$.
    \begin{claim}[]{(1)}
        \[\mathbb{P}(\exists y \in \mathcal{X} : I_{x,y} \geq \eta \Delta -1) \leq \mathbb{E}|B_x(\log d) \cap \mathcal{X}|
        + \mathbb{E}|\{y \in \mathcal{X} \setminus B_x(\log d) : I_{x,y} \geq \eta \Delta - 1\}|\]
    \end{claim}

    \begin{proof}[Proof of claim (1)]
        Observe, by applying Markov's inequality twice,
        \begin{align*}
            &\mathbb{P}(\exists y \in \mathcal{X} : I_{x,y} \geq \eta \Delta - 1) \\
             &= \mathbb{P}(\exists y \in \mathcal{X} \cap B_x(\log d) : I_{x,y} \geq \eta \Delta - 1)
            + \mathbb{P}(\exists y \in \mathcal{X} \setminus B_x(\log d) : I_{x,y} \geq \eta \Delta - 1) \\
            &= \mathbb{P}(|\{y \in \mathcal{X} \cap B_x(\log d) : I_{x,y} \geq \eta \Delta - 1\}| \geq 1) + 
            \mathbb{P}(|\{y \in \mathcal{X} \setminus B_x(\log d) : I_{x,y} \geq \eta \Delta - 1\}| \geq 1) \\
            &\leq \mathbb{E}|\{y \in \mathcal{X} \cap B_x(\log d) : I_{x,y} \geq \eta \Delta - 1\}| + 
            \mathbb{E}|\{y \in \mathcal{X} \setminus B_x(\log d) : I_{x,y} \geq \eta \Delta - 1\}| 
            \tag{\text{Markov}} \\
            &\leq \mathbb{E}|\mathcal{X} \cap B_x(\log d)| + \mathbb{E}|\{y \in \mathcal{X} \setminus B_x(2r_d) : I_{x,y} \geq \eta \Delta - 1\}| 
        \end{align*}
        and we're done :)
    \end{proof}
    Now we'll bound the two terms in our summation. For the leftmost term, simply observe 
    \[\mathbb{E}|\mathcal{X} \cap B_x(\log d)| = \lambda \text{Vol}(B_x(\log d)) 
    = \left(\frac{\sqrt{d}}{8\log d}\right)^d \cdot \left(\frac{\pi^{d/2}}{\Gamma(\frac{d}{2} + 1)}(\log d)^d\right)
    = \frac{\left(\frac{d\pi}{64}\right)^{d/2}}{\Gamma(\frac{d}{2} + 1)}\]
    Which with $\Gamma(d/2 + 1) \geq 2^{-d/2}e^{-d/2}d^{d/2}$ (Stirling's approximation) gives 
    \[\frac{\left(\frac{d\pi}{64}\right)^{d/2}}{\Gamma(\frac{d}{2} + 1)} \leq \left(\frac{2\pi e}{32}\right)^{d/2}\]
    Now Campos $\&$ Co jump from this to a bound above of $\frac{1}{4d}$. I make some comments about this below. \\

    For the rightmost term we'll need the following result. 
    \begin{claim}[]{(2)}
        For $t \geq 0$, take $x, y \in \mathbb{R}^d$ with $\lvert x - y \rvert \geq t$. Then,
        \[\text{Vol}(B_x(2r_d)\cap B_y(2r_d)) \leq 2^d e^{-t^2/4}\]
    \end{claim}
    \begin{proof}[Proof of claim 2]
        Easy calculations, just take a (carefully chosen) covering ball. See \cite{campos2023} fact 2.3.
    \end{proof}
    Now, using Mecke, observe (as $I_{x,y} = 0$ on $y \notin B_{x}(4r_d)$) setting $\Omega^\prime 
    := B_x(4r_d) \setminus B_x(\log d)$
    \begin{align*}
        \mathbb{E}&|\{y \in X \setminus B_x(\log d) : I_{x,y} \geq \eta \Delta - 1\}| 
        = \lambda \int_{\Omega^\prime}\mathbb{P}(X_{x,y} \geq \eta \Delta - 2)\mathrm{d}y \\
        &\leq \lambda \text{Vol}(\Omega^\prime) \; \times \;\max_{y \in \Omega^\prime}\mathbb{P}(X_{x,y} \geq \eta \Delta - 2) \\
        &\leq \lambda \text{Vol}(B_x(4r_d)) \; \times \; \max_{y \in \Omega^\prime}\mathbb{P}(X_{x,y} \geq \eta \Delta - 2) \\
        &= \mathbb{E}[X \cap B_x(4r_d)] \; \times \; \max_{y \in \Omega^\prime}\mathbb{P}(X_{x,y} \geq \eta \Delta - 2)
    \end{align*}
    Now some annoying bounding gives the same $\frac{1}{4d}$ upper bound. Gonna eat lunch then type this up.
\end{proof}

\begin{remark}[]{on claim 1}
    Could we get something tighter by initially summing over $y \in \mathcal{X}$ (disjoint probabilities) and 
    then bounding the terms in the summand? Also the choice of our split set to be $B_x(\log d)$ is purely to 
    cancel with the chosen $\log(d)$ factor in our intensity $\lambda$. Worth noting. 
\end{remark}

\begin{remark}[]{on the bounding of $\mathbb{E}|\mathcal{X} \cap B_x(\log d)|$}
    That final leap seems super lazy? We go from an exponential rate of decrease to linear. Surely this can be 
    improved to achieve something non-trivial? Maybe not, worth investigating regardless. In the paper they get 
    $8$ as their final denominator, I get $32$? Did I improve this or make a mistake? Check it out! 
\end{remark}

Now we may finally prove our original lemma, lemma 2.26 / lemma 2.1 in \cite{campos2023}. 

\begin{proof}[Proof of lemma 2.1]
    Set $S_1$ be the collection of points with high degree and $S_2$ those with high codegree (i.e. the left/right 
    conditions in $(\heartsuit)$) and $X = \mathcal{X} \setminus (S_1 \cup S_2)$ the space without these ``bad'' points.
    Then,
    \[\mathbb{E}|X| \geq \mathbb{E}|\mathcal{X}| - \mathbb{E}|S_1| - \mathbb{E}|S_2| 
    \geq \left(1 - \frac{1}{d}\right)\mathbb{E}|\mathcal{X}| = \left(1 - \frac{1}{d}\right)\text{Vol}(\Omega)\Delta 2^{-d}\]
    using our lemmas for the expected size of $S_1$ and $S_2$, so we have our $X \subset \Omega$ (by $\mathbb{P}(X \geq \mathbb{E}X) > 0$).
\end{proof}

\begin{remark}[]{on the $d \geq 1000$ assumption}
    Where was $d \geq 1000$ used? I can only spot a $d \geq 4$ in the use of Stirling's. Worth checking.
\end{remark}

\subsubsection*{Nibbling on almost-uniform graphs}

We now introduce the so called {\it R\"{o}dl nibble}, which will be the key to our proof of thm 1.3. \\

Throughout this (sub)section, we make the following assumptions. 
\[d_G(v) \in \{\Delta -1, \Delta\}, \; \Delta_{\text{co}}(G) \leq \eta \Delta, \; \Delta \geq 1, \; 
\gamma \leq \frac{1}{2}, \; \eta \in [\Delta^{-1/2}, \gamma^2/8]\]

We spend this section showing that after each ``nibble'', the degrees and codegrees decrease appropriately. This 
result is summarised with the following proposition. 

\begin{proposition}[]{nibbles dont kill degrees / lemma 3.1 in \cite{campos2023}}
    Let $\Delta, \gamma, \eta$ be as above, let $\alpha \in [2\gamma^2, \gamma]$ and let $G$ be as above. Let 
    $A \subseteq V(G)$ be a $(\gamma/\delta)$-random set, and let $G^\prime = G \setminus (A \cup \mathcal{N}_G(A))$.
    Then, $\forall u \neq v \in G$ 
    \begin{align*}
        \mathbb{P}(d_{G^\prime}(v) \geq (1 - \delta + \alpha)d_G(v) | v \in V(G^\prime)) \leq \text{exp}\left(\frac{-\alpha^2}{32\gamma\eta}\right) \\
        \mathbb{P}(d_{G^\prime}(u,v) \geq (1 - \delta + \alpha)\eta\Delta | v \in V(G^\prime)) \leq \text{exp}\left(\frac{-\alpha^2}{32\gamma\eta}\right)
    \end{align*}
\end{proposition}

Before we're able to prove this we'll need some preliminary results concerning our nibble. First, we'll calculate the 
mean degree/codegree after each nibble. 

\begin{lemma}[]{mean degree/codegree post-nibble / lemma 3.2 in \cite{campos2023}}
    Let $\Delta, \gamma, \eta$, $G$ be as defined at the start of the (sub)section, let $A$ and $G^\prime$ be as in 
    prior proposition (lemma 3.1 in \cite{campos2023}). Then, for $u, v \in V(G)$, 
    \[\mathbb{E}[d_{G^\prime}(v) | v \in V(G^\prime)] \leq (1 - \gamma + \gamma^2)d_G(v) \quad \text{and} \quad
    \mathbb{E}[d_{G^\prime}(u, v) | u, v \in V(G^\prime)] \leq (1 - \gamma + \gamma^2)d_G(u, v)\]
\end{lemma}

We just prove the mean degree result, noting the proof for the codegree is almost identical. \\

The idea is to look individually at vertices in $\mathcal{N}_G(v)$, given $v$ is not ``eaten'', and to bound the 
probability this vertex survives the ``nibble''.

\begin{proof}
     Conditioned on the fact $v \in V(G^\prime)$, we know that $A$ is a $(\gamma/\Delta)$-random subset of 
     $V(G) \setminus (\mathcal{N}_G(v) \cup \{v\})$ (as all the $\mathcal{N}_G(v) \cup \{v\}$ must survive). 
     Let $w \in \mathcal{N}_G(v)$ and $d_w = |\mathcal{N}_G(w) \setminus (\mathcal{N}_G(v) \cup \{v\})|$. 
     Then, \[d_w = |\mathcal{N}_G(w)| - |\mathcal{N}_G(v) \cap \mathcal{N}_G(w)| - |\{v\}| 
     = d_G(w) - d_G(w, v) - 1\]
     (where the $-1$ comes from $w \in \mathcal{N}_G(v)$), so by our setup we may bound 
     \[d_w \geq (\Delta - 1) - \eta\Delta - 1 \geq \ (1 - \gamma^2/8)\Delta - 2\]
     [SLIGHTLY OFF ON LAST BOUND, SHOULD ARRIVE AT $(1 - \gamma^2/2)\Delta$]. \\
     For $w \in V(G^\prime)$ to be true (conditioned on $v \in V(G^\prime)$), we need none of the $w^\prime \in 
     \mathcal{N}_G(w) \setminus (\mathcal{N}_G(v) \cup \{v\})$ to be ``eaten''. Hence, 
     \[\mathbb{P}(w \in V(G^\prime) | v \in V(G^\prime)) = \left(1 - \frac{\gamma}{\Delta}\right)^{d_w}\]
     which, using $d_w \in \mathbb{N}_0$ and the Binomial theorem, gives 
     \[\mathbb{P}(w \in V(G^\prime) | v \in V(G^\prime)) \leq 1 - \frac{\gamma}{\Delta}d_w + \frac{\gamma^2}{2\Delta^2}d_w
     \leq 1 - \gamma + \gamma^2\] where we used $(1-\gamma^2/2)\Delta \leq d_w \leq \Delta$ in the final deduction. 
     Now we may just sum over $w \in \mathcal{N}_G(v)$ to obtain the final result.
\end{proof}

We now state a one sided version of Freedman's inequality (a martingale generalisation of one of the Bernstein 
inequalities), courtesy of Chung $\&$ Lu. 

\begin{proposition}[]{Chung $\&$ Lu concentration inequality / Theorem 3.3 in \cite{campos2023}}
    Let $(S_i)_{i=0}^n$ be a martingale with respect to a filtration $(\mathcal{F}_i)_{i=0}^n$. Suppose $(S_i)$ 
    has increments $(\xi_j)_{j=1}^n$ with $\xi_j \leq R_j$ and $\mathbb{E}[|\xi_j|^2 | \mathcal{F}_j] \leq \sigma_i^2$ 
    almost surely. Then, for all $a \geq 0$, \[\mathbb{P}(S_n - S_0 \geq a) 
    \leq \exp\left(-\frac{a^2}{2\sum_{i=1}^n(R_i^2 + \sigma_i^2)}\right)\]
\end{proposition}

\newpage

\section{Appendices}

% Count over naturals
\setcounter{lemmacount}{1}
\setcounter{examplecount}{1}
\setcounter{theoremcount}{1}
\setcounter{propositioncount}{1}
\setcounter{corollarycount}{1}
\setcounter{remarkcount}{1}
\setcounter{definitioncount}{1}

\subsection{Appendix A: Poisson Point Processes}

For a full treatment, consult \cite{Last_Penrose_2017}. Here I will simply give the relevent definitions and 
results from this text, leaving proofs (unless containing a particularly important idea) to \cite{Last_Penrose_2017}.

\begin{definition}[]{point process}
    Let $(\mathbb{X}, \mathcal{X})$ be a measure space and let ${\bf N}(\mathbb{X}) \equiv {\bf N}$ be the family 
    of measures that can be written as a countable sum of finite measures on $\mathbb{X}$ with image in $\mathbb{N}_0$. 
    Let ${\bf \mathcal{N}}(\mathbb{X}) \equiv {\bf \mathcal{N}}$ be the $\sigma$-algebra generated by the collection 
    of subsets $\{\mu \in {\bf N} : \mu(A) = k\}$ over $A \in \mathcal{X}, k \in \mathbb{N}_0$. A {\it point process}
    on $\mathbb{X}$ is a random element $\eta$ of the measure space $({\bf N}, {\bf \mathcal{N}})$
\end{definition}

\begin{definition}[]{Poisson point process}
    Let $\mathbb{X}$ be a space and $\lambda$ an s-finite measure on $\mathbb{X}$. A {\it Poisson point process} 
    with intensity measure $\lambda$ is a point process $\eta$ on $\mathbb{X}$ with
    \begin{enumerate}[(i)]
        \item $\eta(A) \sim \text{Poisson}(\lambda(A))$, that is $\eta(A)$ is Poisson with parameter $\lambda(A)$
        \item If $A_1, \dots, A_n \in \mathbb{X}$ are pairwise disjoint then $\eta(A_1), \dots, \eta(A_n)$ 
        are independent.
    \end{enumerate}
\end{definition}

\newpage

\subsection{Appendix B: Existence Of High Density Packings}

Here I take some notes on chapter 7 of Pach $\&$ Agarwals combinatorial geometry [WILEY GIVES BAD CITATION], 
hoping to find bridges between random geometric graphs and combinatorial geometry. \\ 

As the title suggests, we spend this section looking for the existence of high density packings in $\mathbb{R}^d$. 
We take a non construct approach, instead turning to the probabilistic method. For a convex body $C \subset \mathbb{R}^d$, 
denote the maximal packing (respectively, minimal covering) density by $\delta(C)$ (respectively, $\vartheta(C)$). \\

Let us first consider $C = B^d \subset \mathbb{R}^d$, the $d$-dimensional ball. Then one has the following easy 
observation. 

\begin{proposition}[]{easy lower bound for $\delta(B^d)$}
    Let $B^d \subset \mathbb{R}$ be the $d$-dimensional unit ball. Then, 
    \[\delta(B^d) \geq 2^{-d}\vartheta(B^d) \geq 2^{-d}\]
\end{proposition}

\begin{proof}
    The latter inequality is trivial ($\vartheta(C) \geq 1$). For the former, let us consider a {\it saturated 
    packing} $\mathcal{B} = \{B^d + c_i | i \in \mathbb{N}\}$ (that is, a packing such that 
    no body can be added without intersecting a pre-existing body) and the factor 2 enlargement 
    $\mathcal{B}^\prime = \{2B^d + c_i | i \in \mathbb{N}\}$.
    \begin{claim}
        $\mathcal{B}^\prime$ is a covering of $\mathbb{R}^d$.
    \end{claim}
    \begin{proof}[Proof of claim]
        Suppose $x \notin \cup_{B \in \mathcal{B}^\prime} B$. Then we have $\lvert x - c_i \rvert > 2$ for som $i \geq 1$, 
        which gives \[(B^d + c_i) \cap (B^d + x) = \emptyset\] Contradiction!
    \end{proof}
    Hence, letting $d(C, \mathbb{R}^d)$ denote the density of a body $C$ in $\mathbb{R}^d$, we clearly have 
    \[\vartheta(B^d) \leq d(\mathcal{B}^\prime, \mathbb{R}^d) = 2^d d(\mathcal{B}, \mathbb{R}^d) \leq 2^d \delta(B^d)\] 
    and the result is proven. 
\end{proof}

In high dimensional Euclidean space, this trivial bound was the best bound known, up to a constant, for a long 
time! In fact, no construction of saturated lattice packings of balls is known (again, in high dimensional space), worth 
checking if this has changed since time of writing. This is surprising, given the obvious drawbacks of this argument. 
If we saturate a packing by continuously adding balls, we can't control the structure of this packing. \\ 

Let $\delta_L(C)$ be the maximal lattice packing density of $C$ into $\mathbb{R}^d$. Using techniques developed by 
Minkowski and Hlawka, we can prove $\delta_L(B^d) > 2^{-d}$. Namely, we can affirm the existence of a lattice packing 
strictly outperforming our previously derived lower bound of $2^{-d}$.  

\begin{definition}[]{star-shaped body}
    Let $C \subset \mathbb{R}^d$ be compact. We call $C$ a {\it star-shaped body} if ${\bf 0}$ is in the interior 
    of $C$ and has the relation ${\bf x} \in C \Rightarrow \lambda {\bf x} \in C$ for all $\lambda \in [0, 1]$.
\end{definition}

\begin{definition}[]{admissible latice}
    Let $\Lambda = \Lambda(u_1, \dots, u_d) = \{m_1 u_1 + \cdots + m_d u_d | m_1, \dots, m_d \in \mathbb{Z}\}$ be a 
    lattice in $\mathbb{R}^d$ and let $C$ be a star-shaped body. Then $\Lambda$ is said to be {\it admissible} for 
    $C$ if it contains no interior point of $C\setminus \{{\bf 0}\}$.
\end{definition}

\begin{definition}[]{critical determinant}
    Let $C$ be a star-shaped body. The {\it critical determinant} of $C$ is defined by 
    \[\Delta(C) := \inf \{\det \Lambda | \Lambda \text{ is admissible for } C\}\]
\end{definition}

\begin{lemma}[]{Mahler selection theorem - 1946}
    Let $\Lambda_1, \Lambda_2, \dots$ be an infinite sequence of lattices in $\mathbb{R}^d$ that have constants 
    $\alpha, \beta > 0$ satisfying, for $i \geq 1$, 
    \begin{enumerate}[(i)]
        \item $\Lambda_i$ is admissible for $\alpha B^d$
        \item $\det \Lambda_i \leq \beta$
    \end{enumerate}
    Then we can select a convergent subsequence $\Lambda_{i_1}, \Lambda_{i_2}, \dots \to \Lambda$.
\end{lemma}

\begin{proof}
    Given as exercise, solve at some point.
\end{proof}

This lemma allows us to prove that the infimum is obtained (verify this also), giving us the following reformulation
of Minkowski's fundamental theorem (or as I prefer, Minkowski's pidgeonhole principle).

\begin{theorem}[]{Minkowski's critical determinant lower bound}
    For any centrally symmetric convex body $C \subset \mathbb{R}^d$, 
    \[\frac{\Delta(C)}{\text{vol}C} \geq \frac{1}{2^d}\]
\end{theorem}

Finding an upper bound for the ratio $\Delta(C)/\text{vol}C$ turns out the be much harder. Minkowski was able to 
give an upper bound, when $C = B^d$, of $\frac{1}{2}(\zeta(d))^{-1}$ where $\zeta(\cdot)$ is the Reimman zeta function.
We prove, thanks to Hlawka, a generalisation of Minkowski's result. 

\begin{lemma}[]{Davenport $\&$ Rogers}
    Let $f:\mathbb{R}^d \to \mathbb{R}$ be a continuous map that vanishes outside of some bounded region. For $\gamma
    \in \mathbb{R}$ set \[V(\gamma) := \int_{\mathbb{R}}\cdots\int_{\mathbb{R}}f(x_1, \dots, x_{d-1}, \gamma)\mathrm{d}x_1 \cdots \mathrm{d}x_{d-1}\] 
    Fix $\delta > 0$ and let $\Lambda^\prime$ be the integer lattice in the hyperplane $x_d = 0$. For a vector $y \in \mathbb{R}^d$ 
    of the form $y = (y_1, \dots, y_{d-1}, \delta)$ define $\Lambda_y$ as the lattice generated by $y$ and $\Lambda^\prime$. 
    Then, \[\int_{0}^1 \cdots \int_{0}^1 \left(\sum_{x \in \Lambda_y, x_d \neq 0}f(x)\right)\mathrm{d}y_1 \cdots \mathrm{d}y_{d-1} 
    = \sum_{i \in Z\setminus \{0\}}V(i \delta)\]
\end{lemma}

\begin{proof}
    Looks like a routine computation but theres one line dependent on voodoo magic, can't wrap my head around that 
    bloody jump, but I imagine its something dumb I'm missing :(
\end{proof}

Now for our generalisation. We work, as suggested, with the probabilistic method. 

\begin{theorem}[]{Hlawka's theorem}
    Let $f: \mathbb{R}^d \to \mathbb{R}$ be a bounded Reimann integrable function that vanishes outside of some bounded
    region and let $\epsilon > 0$. Then there exists a unit lattice $\Lambda$ in $\mathbb{R}^d$ with 
    \[\sum_{{\bf 0} \neq x \in \Lambda}f(x) < \int_{\mathbb{R}^d}f(x)\mathrm{d}x + \epsilon\]
\end{theorem} 

\begin{proof}
    Skill issue + lack of relevance. Moving onto non-lattice packings, if this becomes interesting again I'll revisit.
\end{proof}

\newpage

%%% References
\bibliographystyle{plain}
\bibliography{refs} 

\end{document} 