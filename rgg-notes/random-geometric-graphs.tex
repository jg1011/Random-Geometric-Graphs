\documentclass{article} 

\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\usepackage{enumerate}
\usepackage[a4paper, margin=1in]{geometry} 
\usepackage{titling}
\usepackage{amssymb}
\usepackage{lipsum}
\usepackage{mathtools}
\usepackage{amsthm} % For proof environments only, labelling of thms done with custom counter

%%% Misc commands %%%
\newcommand\iidsim{\stackrel{\mathclap{\tiny\mbox{i.i.d}}}{\sim}}
\newcommand\indist{\stackrel{\mathclap{\tiny\mbox{$\mathcal{D}$}}}{\longrightarrow}}
\newcommand\nidist{\stackrel{\mathclap{\tiny\mbox{$\mathcal{D}$}}}{\longleftarrow}}
\newcommand{\indep}{\perp\!\!\!\!\perp} 

\setlength{\parindent}{0pt} % Remove indentation upon new paragraph. 

%%% Title page information %%%
\title{Random Geometric Graphs}
\author{Jacob Green}
\date{\today}
\newcommand{\subtitle}{A collection of notes from my time working under the supervision of Mathew Penrose on 
random geometric graph theory and related topics.}
\newcommand{\institution}{Department of Mathematical Sciences, The University of Bath}
\newcommand{\keywords}{Random geometric graphs, Poisson Point Processes}

\newcounter{globaltcb} % tcolorbox counter, for ease of self-reference
\newcounter{claimcount} % Used for proof with lots of claims

\newtcolorbox{definition}[2][use counter=globaltcb, number within=section]{
  colback=black!5!white, 
  colframe=black!50!white, 
  fonttitle=\bfseries, 
  coltitle=black,
  title=Definition \arabic{section}.\arabic{globaltcb} (#2), 
  before upper={\refstepcounter{globaltcb}},
  #1, % Optional params
}

\newtcolorbox{lemma}[2][use counter=globaltcb, number within=section]{
  colback=blue!5!white, 
  colframe=blue!50!white, 
  fonttitle=\bfseries, 
  coltitle=black, 
  title=Lemma \arabic{section}.\arabic{globaltcb} (#2), 
  before upper={\refstepcounter{globaltcb}},
  #1, % Optional params
}

\newtcolorbox{example}[2][use counter=globaltcb, number within=section]{
  colback=green!5!white, 
  colframe=green!50!white, 
  fonttitle=\bfseries, 
  coltitle=black, 
  title=Example \arabic{section}.\arabic{globaltcb} (#2), 
  before upper={\refstepcounter{globaltcb}},
  #1, % Optional params
}

\newtcolorbox{theorem}[2][use counter=globaltcb, number within=section]{
  colback=red!5!white,
  colframe=red!50!white, 
  fonttitle=\bfseries, 
  coltitle=black, 
  title=Theorem \arabic{section}.\arabic{globaltcb} (#2), 
  before upper={\refstepcounter{globaltcb}},
  #1, % Optional params
}

\newtcolorbox{proposition}[2][use counter=globaltcb, number within=section]{
  colback=purple!5!white,
  colframe=purple!50!white, 
  fonttitle=\bfseries, 
  coltitle=black, 
  title=Proposition \arabic{section}.\arabic{globaltcb} (#2), 
  before upper={\refstepcounter{globaltcb}},
  #1, % Optional params
}

\newtcolorbox{corollary}[2][use counter=globaltcb, number within=section]{
  colback=yellow!5!white,
  colframe=yellow!50!white, 
  fonttitle=\bfseries, 
  coltitle=black, 
  title=Corollary \arabic{section}.\arabic{globaltcb} (#2), 
  before upper={\refstepcounter{globaltcb}},
  #1, % Optional params
}

\newtcolorbox[use counter=claimcount]{claim}[1][]{%
    colback=green!10, 
    colframe=green!50!black, 
    coltitle=black, 
    fonttitle=\bfseries, 
    boxrule=0.5mm, 
    colbacktitle=green!30,
    enhanced, % Allows additional options
    boxed title style={sharp corners}, 
    attach title to upper={},
    titlerule=0mm, 
    title=Claim: $\;$,
    before=\par\smallskip\noindent, 
    #1
}

\begin{document}

\begin{titlepage}
    \centering
    % Adding an image (optional)
    
    % Title
    {\Huge \bfseries \thetitle \par}
    \vspace{0.5cm}
    
    % Subtitle (if any)
    {\Large \subtitle \par}
    \vspace{1cm}
    
    % Author and institution
    {\large \theauthor \par}
    {\institution \par}
    \vspace{1cm}
    
    % Date
    {\large \thedate \par}
    \vspace{1.5cm}
    
    % Abstract section
    \begin{abstract}
        \lipsum[10]
    \end{abstract}
    \vspace{1cm}
    
    % Keywords section
    \textbf{Keywords:} \keywords
    \vfill % Pushes the following content to the bottom
    
    % Footer or further information
    \textit{}
\end{titlepage}

\newpage

\tableofcontents 

\newpage 

\setcounter{page}{1} % Start page numbering from 1 after title page

\section{Introduction}

\setcounter{globaltcb}{1} % Reset box counter.

\subsection{What is a random geometric graph?}

\begin{definition}[]{geometric graph}
    Let $\mathcal{X} \subset \mathbb{R}^d$ be a finite collection of points and $r > 0$. The {\it geometric graph}
    $G(\mathcal{X}, r)$ is the graph with vertex set $\mathcal{X}$ and edge set $\{\{x, y\} \subset \mathcal{X} 
    \; : \; \lvert x - y \rvert \leq r\}$, where $\lvert \cdot \rvert$ denotes the Euclidean norm. 
\end{definition}

We can turn this into a random graph by letting the set $\mathcal{X} \subset \mathbb{R}^d$ be random. The resulting
structure $G(\mathcal{X}, r)$ is said to be the {\it random geometric graph} (A.K.A the {\it Gilbert graph}). The first 
structure we'll consider is found by uniformly scattering $n$ points in the $d$-dimensional hypercube $[0,1]^d$. 

\begin{example}[]{fixed scale geometric graph}
    Let $\xi_1, \xi_2, \dots \iidsim \text{Unif}[0,1]^d$ and fix a sequence $(r_n)_{n \geq 1}$ in $\mathbb{R}_{>0}$.
    Let $\mathcal{X}_n := \{\xi_1, \dots, \xi_n\}$. Then $G(\mathcal{X}_n, r_n)$ is the {\it fixed scale geometric 
    graph} at time $n \geq 1$.
\end{example}

We will also consider what happens when the number of vertices is Poisson.

\begin{example}[]{Poisson scale geometric graph}
    Let $\xi_1, \xi_2, \dots \iidsim \text{Unif}[0,1]^d$ and $N_n \sim \text{Poisson}(n)$ for $n \geq 1$ be independent 
    of $(\xi_1, \xi_2, \dots)$. Let $\mathcal{P}_n := \{\xi_1, \dots, \xi_{N_n}\}$ and fix a sequence $(r_n)_{n \geq 1}$
    in $\mathbb{R}_{> 0}$. Then the random geometric graph $G(\mathcal{P}_n, r_n)$ is the $d$-dimensional {\it Poisson 
    scale geometric graph} at time $n \geq 1$. 
\end{example}

As one may expect, the vertices of this graph form a Poisson point process in $\mathbb{R}^d$. This was originally 
an exercise (1.1) in \cite{Penrose_et_al_2016}.

\begin{proposition}[]{$\mathcal{P}_n(\cdot)$ is a Poisson point process}
    Let $\mathcal{P}_n$ be the vertex set in the Poisson scale geometric graph. Then $\mathcal{P}_n(\cdot): A \mapsto 
    |\mathcal{P}_n \cap A|$, where $A \subset [0,1]^d$ is measurable and $|\cdot|$ is the counting measure, is a 
    {\it Poisson point process} with intensity $\lambda(A)$, $\lambda(\cdot)$ the Lebesgue measure on $\mathbb{R}^n$.
\end{proposition}

See Appendix A for the definition of the Poisson point process. 

\begin{proof}
Write $\mathcal{P}_n(A_j) = \sum_{i=1}^{N_n} {\bf 1}\{\xi_i \in A_j\}$. Then, conditioning on $\{N_n = m\}$, each 
$\mathcal{P}_n(A_j)$ is a $\text{Binom}(m, \lambda(A_j))$ random variable, making the joint distribution (also 
conditioned on $\{N_n = m\}$) a $\text{Multinom}(m, \lambda(A_1), \dots, \lambda(A_k))$. Thus one has 
\begin{align*}\mathbb{P}(\mathcal{P}_n(A_1) = a_1, \dots, \mathcal{P}_n(A_k) = a_k) &= 
\underbrace{\left(\frac{e^{-n}n^m}{m!}\right)}_{\text{Poisson}(n ; m)} \quad \times 
\underbrace{\binom{m}{j_1, \dots, j_k}\prod_{i=1}^k {(\lambda(A_i))}^{j_i}}_{\text{Multinom}(m, \lambda(A_1), \dots, \lambda(A_k) ; j_1, \dots, j_k)} \\
&= \prod_{i=1}^k \frac{e^{-n\lambda(A_i)}(n\lambda(A_i))^{j_i}}{(j_i)!}
\end{align*}
which is a product of the distributions $\text{Poisson}(n\lambda(A_i)), 1 \leq i \leq k$, giving us both of our 
defining properties of a Poisson point process. 
\end{proof}

The exercise (1.2) in \cite{Penrose_et_al_2016} states the following result, which we'll prove in an analogous way.

\begin{proposition}[]{expected degree of $k^\text{th}$ vertex in fixed scale RGG}
    Consider the fixed scale random geometric graph $G(\mathcal{X}_n, r_n)$ with $r_n \to 0$ as $n \to \infty$. Let 
    $D_{k, n}$ be the degree of the first vertex $\xi_k$, then $\mathbb{E}[D_{k, n}] \sim \theta n r_n^d$.
\end{proposition}

\begin{proof}
Write $D_{k,n} = \sum_{i \neq k} {\bf 1}\{\lvert \xi_k - \xi_i \rvert \leq r\}$, then we have $D_{k,n} \sim 
\text{Binom}(n-1, p)$ with \[p = \lambda(B_{r_n}(\xi_k) \cap [0,1]^d)\] where $B_{r_n}(\xi_k)$ is the hypersphere 
radius $r_n$ centred at $\xi_k$ and $\lambda$ the Lebesgue measure on $\mathbb{R}^d$. Now, conditioning on $\xi_k$, 
we have \[\mathbb{E}[D_{k,n} | \xi_k] = (n-1)\lambda(B_{r_n}(\xi_k) \cap [0,1]^d)\]
and hence, by the total law of expectation, 
\begin{align*}
    \mathbb{E}[D_{k,n}] &= \mathbb{E}[\mathbb{E}[D_{k,n} | \xi_k]] \\
    &= (n-1)\int_{[0,1]^d}\lambda(B_{r_n}(\xi_k) \cap [0,1]^d)\mathrm{d}\xi_k \\
    &\sim n\int_{[0,1]^d}\lambda(B_{r_n}(\xi_k))\mathrm{d}\xi_k \sim n(\theta r_n^d)
\end{align*}
Where the first asymptotic equality follows via the dominated convergence theorem (to see this, observe $B_{r_n}(\xi_k) \cap [0,1]^d 
\to B_{r_n}(\xi_k)$ A.E. and $B_{r_n}(\xi_k)$ also dominating). Note $\theta$ is a constant depending on $d$, 
and is the coefficient of $r_n^d$ in the formula for the volume of a hypersphere radius $r_n$.
\end{proof}

\subsection{Counting Edges of $G(\mathcal{X}_n, r_n)$ and $G(\mathcal{P}_n, r_n)$}

The number of edges $\mathcal{E}_n, \mathcal{E}_n^\prime$ in $G(\mathcal{X}_n, r_n)$ and $G(\mathcal{P}_n, r_n)$ respectively 
is a Poisson random variable. To prove this, we'll use a so called {\it dependency graph}. We handle the two 
RGGs $G(\mathcal{X}_n, r_n)$ and $G(\mathcal{P}_n, r_n)$ seperately. 

\subsubsection*{Counting edges of $G(\mathcal{X}_n, r_n)$}

An elementary question to ask is "what is the expected edge count at time $n$?". Via the previously derived 
vertex degree asymptotics, we have an easy asymptotic result.  

\begin{proposition}[]{expected edge count in fixed scale RGG}
    Let $\mathcal{E}_n$ and $\mathcal{E}^\prime_n$ be the number of edges in $G(\mathcal{X}_n, r_n)$ and 
    $G(\mathcal{P}_n, r_n)$ respectively. Then,
    \[\mathbb{E}[\mathcal{E}_n] \sim \theta n^2 r_n^d / 2\]
\end{proposition}

\begin{proof}[Proof of lemma]
    By the handshaking lemma we have $2\mathbb{E}[\mathcal{E}_n] = \sum_{k = 1}^n \mathbb{E}[D_{k, n}] \sim n^2 r_n^d$. 
\end{proof}

Now we introduce the {\it dependency graph}. 

\begin{definition}[]{dependency graph}
    Let $(V, \sim)$ be a finite simple graph w/ edge relation $\sim$ and vertex set $V$. We say $(V, \sim)$ is a 
    {\it dependency graph} for the random variables $(W_\alpha)_{\alpha \in V}$ if whenever $A, B \subset V$ are 
    disjoint with no $\alpha \in A$, $\beta \in B$ such that $\alpha \sim \beta$ (i.e. $A$ and $B$ lie in unique 
    connected components) then \[(W_\alpha)_{\alpha \in A} \indep (W_\beta)_{\beta \in B} \quad \text{i.e. are independent families}\]
\end{definition}

When we take the random variable associated with each vertex to be 0-1, we obtain the following bound. 

\begin{lemma}[]{Poisson approximation lemma for Bernoulli sums}
    Let $(\xi_i)_{i \in I}$ be a finite collection of Bernoulli random variables with dependency graph $(I, \sim)$. 
    Set $p_i := \mathbb{P}(\xi_i = 1)$, $p_{ij} := \mathbb{P}(\xi_i = 1 ; \xi_j = 1)$ and suppose $\lambda := 
    \sum_{i \in I}p_i$ is finite. Then, letting $W := \sum_{i \in I}\xi_i$, we have (LHS is just total variation distance)
    \[\sum_{k \geq 0} \bigg|\mathbb{P}[W = k] - \mathbb{P}(\text{Po}(\lambda) = k)\bigg| \leq 
    \min \{6, 2\lambda^{-1}\} \left(\sum_{i \in I}\sum_{\mathcal{N}(i) \setminus \{i\}}p_{ij} + 
    \sum_{i \in I}\sum_{j \in \mathcal{N}(i)}p_ip_j\right)\]
\end{lemma}

\begin{proof}
    Omitted, consult \cite{Penrose_2003}.
\end{proof}

\begin{theorem}[]{total variation between edge and Poisson distribution}
    Let $\lambda_n := \mathbb{E}[\mathcal{E}_n]$. Then 
    \[\sum_{k \geq 0} | \mathbb{P}(\mathcal{E}_n = k) - \mathbb{P}(\text{Po}(\lambda_n) = k) | = O(nr^d)\]
\end{theorem}

\setcounter{claimcount}{1} % Using claims, reset the count

The idea of this proof is to write $\mathcal{E}_n = \sum_{1 \leq i < j \leq n} {\bf 1}\{\lvert \xi_i - \xi_j \rvert \leq r_n\}$,
i.e. the number of edges as the sum of the indicators over all possible edges indicating whether this edge 
exists. This has a rather obvious dependency graph and hence we can apply the Poisson approximation lemma (1.7)
with some work on the asymptotics.

\begin{proof}
Let $V \{\{i, j\} : 1 \leq i < j \leq\}$ and define $\sim$ by $\alpha \sim \beta$ if $\alpha \cap \beta \neq \emptyset$ 
and $\alpha \neq \beta$. Define the random variables $W_\alpha = {\bf 1}\{\lvert \xi_i - \xi_j \rvert \leq r_n\}$ for 
$\alpha \in V$ and let $\lambda_n := \sum_{\alpha \in V}p_\alpha$.

\begin{claim}[]{}
    $G(V, \sim)$ is a dependency graph for $(W_\alpha)_{\alpha \in V}$.
\end{claim}

\begin{proof}[Proof of claim]
    Follows immediately from the independence of the $\xi_i$.
\end{proof}

Let $p_\alpha := \mathbb{P}(W_\alpha = 1)$ and $p_{\alpha \beta} := \mathbb{P}(W_\alpha = 1 ; W_\beta = 1)$, as in 
the setup of lemma 1.7.

\begin{claim}[]{}
    $p_\alpha \sim \theta r_n^d$
\end{claim}

\begin{proof}[Proof of claim]
    Fix $\alpha = \{i, j\} \in V$. Then, letting $\lambda(\cdot)$ be the Lebesgue measure on $\mathbb{R}^d$, 
    \begin{align*}
        p_\alpha = \mathbb{P}(\lvert \xi_i - \xi_j \rvert \leq r_n) 
        &= \int_{v \in [0,1]^d} \mathbb{P}(\lvert \xi_i - \xi_j \rvert \leq r_n | \xi_i = v)\mathrm{d}v \\ 
        &= \int_{v \in [0,1]^d} \lambda(B_{r_n}(v) \cap [0,1]^d) \mathrm{d}v \\
        &\sim \int_{v \in [0,1]^d} \lambda(B_{r_n}(v))\mathrm{d}v \sim \theta r_n^d
    \end{align*}
    where the first asymptotic equality follows from the dominated convergence theorem.
\end{proof}

\begin{claim}[]{}
    $p_{\alpha \beta} \sim (\theta r_n^d)^2$
\end{claim}

\begin{proof}[Proof of claim]
    An analogous argument works. 
\end{proof}

Now we have all our ingredients, lets cook. Observe, from the previously computed asymptotics, 
\[\lambda_n \sim n^2\theta r_n^d/2 \quad \text{and} \quad \sum_{\alpha \in V}p_\alpha^2 \sim \theta \lambda_n r_n^d\]
and hence by counting $|\mathcal{N}(\alpha)| = 2(n-2)$, obtain 
\[\sum_{\alpha \in V}\sum_{\beta \sim \alpha}(p_{\alpha \beta} + p_\alpha p_\beta) \sim 
\binom{n}{2} \times 2(n-2) \times 2(\theta r_n^d)^2 = O(n\lambda_n r_n^d)\]
which by the Poisson approximation lemma gives
\[\sum_{k \geq 0}|\mathbb{P}(\mathcal{E}_n = k) - e^{-\lambda_n}\lambda_n^k/k!| = O(nr_n^d)\]  
as claimed. 
\end{proof}

\begin{corollary}[]{edge distribution of the fixed scale random geometric graph}
    If the limit $\lambda_n \to \lambda \in (0,\infty)$ exists and $nr_n^d \to 0$, then 
    $\mathcal{E}_n \indist \text{Poisson}(\lambda)$
\end{corollary}

\begin{proof}
    By proposition 1.6, we have $\lambda \sim \lambda_n \sim \theta n^2 r_n^d/2 \Rightarrow n^2 r_n^d \sim 2\lambda/\theta 
    \in (0, \infty)$ which forces $n r_n^d \to 0$ and hence lemma 1.8 applies giving $\mathcal{E}_n \indist \text{Poisson}(\lambda)$.
\end{proof}

\subsubsection*{Counting edges of $G(\mathcal{P}_n, r_n)$}

We now spend the rest of the (sub)section proving similar results for $G(\mathcal{P}_n, r_n)$. Before proceeding we have
the following lemma from \cite{Penrose_et_al_2016}. We write $\mathcal{P}_{< \infty}(A)$ for the family of finite subsets of $A$. 

\begin{lemma}[]{Mecke formula}
    Let $k \in \mathbb{N}$. For any measurable $f: (\mathbb{R}^d)^k \times \mathcal{P}_{< \infty}([0,1]^d) \to \mathbb{R}$, 
    when the expectation exists, 
    \[\mathbb{E}\sum_{X_1, \dots, X_k \in \mathcal{P}_n}^{\neq} f(X_1, \dots, X_k, \mathcal{P}_n \setminus \{X_1, \dots, X_n\})
    = n^k \int \dots \int \mathbb{E}f(x_1, \dots, x_k, \mathcal{P}_n)\mathrm{d}x_1 \dots \mathrm{d}x_k\]
    where $\sum^{\neq}$ denotes the sum over the ordered $k$-tuples of distinct points (in $\mathcal{P}_n$).
\end{lemma}

\begin{proof}
The idea is to condition on the number of points in $\mathcal{P}_n$. Observe, 
\begin{align*}
    \mathbb{E}&\sum_{X_1, \dots, X_k \in \mathcal{P}_n}^{\neq} f(X_1, \dots, X_k, \mathcal{P}_n \setminus \{X_1, \dots, X_n\}) \\
    &= \mathbb{E}\left[\mathbb{E}\left[\sum_{X_1, \dots, X_k \in \mathcal{P}_n}^{\neq} f(X_1, \dots, X_k, \mathcal{P}_n \setminus \{X_1, \dots, X_n\}) \bigg| |\mathcal{P}_n| = m\right]\right] \\
    &= \sum_{m \geq k}\left(\frac{e^{-n}n^m}{m!}\right)\sum_{X_1, \dots, X_k \in \{\xi_1, \dots, \xi_m\}}^{\neq}\mathbb{E}f(X_1, \dots, X_k, \{X_{k+1}, \dots, X_{m}\}) \\
    (\dagger) \qquad \qquad &= \sum_{m \geq k}\left(\frac{e^{-n}n^m}{m!}\right)\left(\prod_{i = 1}^{k}(m-i+1)\right)\mathbb{E}f(X_1, \dots, X_k, \{X_{k+1}, \dots, X_{m}\}) \\ 
    &= \sum_{m \geq k}\left(\frac{e^{-n}n^m}{m!}\right)\left(\prod_{i = 1}^{k}(m-i+1)\right)\int_{[0,1]^d} \dots \int_{[0,1]^d} f(x_1, \dots, x_k, \{x_{k+1}, \dots, x_m\})\mathrm{d}x_1 \dots \mathrm{d}x_m \\
    &= n^k \sum_{m \geq k}\left(\frac{e^{-n}n^{m-k}}{(m-k)!}\right) \int_{[0,1]^d} \dots \int_{[0,1]^d} f(x_1, \dots, x_k, \{x_{k+1}, \dots, x_m\})\mathrm{d}x_1 \dots \mathrm{d}x_m \\
    &= n^k \int_{[0,1]^d} \dots \int_{[0,1]^d} f(x_1, \dots, x_k, \{x_{k+1}, \dots, x_m\})\mathrm{d}x_1 \dots \mathrm{d}x_m  \\
    &= n^k \int_{[0,1]^d} \dots \int_{[0,1]^d} \mathbb{E}f(x_1, \dots, x_k, \mathcal{P}_n)\mathrm{d}x_1 \dots \mathrm{d}x_k
\end{align*}
Where $(\dagger)$ follows from the $\xi_i$ being i.i.d, so all expectations are equal and it suffices to count 
distinct $k$-tuples in $\{\xi_1, \dots, \xi_m\}$. [CHECK THIS PROOF W/ PENROSE]
\end{proof}

We can now use this lemma to find the expected number of edges in $G(\mathcal{P}_n, r_n)$. 

\begin{proposition}[]{expected edge count in Poisson scale RGG}
    Let $G(\mathcal{P}_n, r_n)$ be the Poisson scale RGG and $\mathcal{E}_n$ be the number of edges at time $n$. Then, 
    \[\mathbb{E}[\mathcal{E}_n] \sim \theta n^2 r_n^d / 2\]
\end{proposition}

\begin{proof}
Write $\mathcal{E}_n = \frac{1}{2}\sum_{X_1, X_2 \in \mathcal{P}_n}^{\neq}{\bf 1}\{\lVert X_1 - X_2 \rVert \leq r_n\}$. Then 
we can use Mecke's formula as follows. 
\begin{align*}
    \mathbb{E}[\mathcal{E}_n] &= \frac{1}{2}\mathbb{E}\left[\sum_{X_1, X_2 \in \mathcal{P}_n}^{\neq}{\bf 1}\{\lVert X_1 - X_2 \rVert \leq r_n\}\right] \\
    (\text{Mecke}) \quad &= \frac{n^2}{2}\int_{[0,1]^d}\int_{[0,1]^d}\mathbb{E}[{\bf 1}\{\lVert x_1 - x_2 \rVert \leq r_n\}]\mathrm{d}x_1\mathrm{d}x_2 \\
    &= \frac{n^2}{2}\int_{[0,1]^d}\int_{[0,1]^d}{\bf 1}\{\lVert x_1 - x_2 \rVert \leq r_n\}\mathrm{d}x_1\mathrm{d}x_2 \\
    &= \frac{n^2}{2}\int_{[0,1]^d}\lambda(B(x_1, r_n) \cap [0,1]^d)\mathrm{d}x_1 \sim \theta n^2 r_n^d / 2
\end{align*}
where the final asymptotic equality follows from the dominated convergence theorem and volume of a $d$-dimensional 
hypersphere. Note we have already seen the asymptotics for the final integral.
\end{proof}

Rest follows on from Exercise (2.2), too small brained to figure it out right now will attempt again tomorrow $:<$

\subsection{A central limit theorem for our edge counts}

\begin{lemma}[]{Falling factorial moment for the Poisson distribution}
    Let $X \sim \text{Poisson}(\lambda)$ and fix $k \in \mathbb{N}$. Then $\mathbb{E}[X(X-1)\dots (X-k+1)] = \lambda^k$
\end{lemma}

\begin{proof}
    This follows by the following simple calculation.
    \begin{align*}
        \mathbb{E}[&X(X-1)\cdots (X - k + 1)] = \sum_{x \geq 0}x(x-1)\cdots (x-k+1)\frac{e^{-\lambda}\lambda^x}{x!} \\ 
        &= e^{-\lambda}\sum_{x \geq k}x(x-1)\cdots (x-k+1)\frac{\lambda^x}{x!} = \lambda^ke^{-\lambda}\sum_{x \geq k}
        \frac{\lambda^{x-k}}{(x-k)!} = \lambda^k
    \end{align*}
\end{proof}

\newpage

\section{Appendices}

\subsection{Appendix A: Poisson Point Processes}

For a full treatment, consult \cite{Last_Penrose_2017}. Here I will simply give the relevent definitions and 
results from this text, leaving proofs (unless containing a particularly important idea) to \cite{Last_Penrose_2017}.

\begin{definition}[]{point process}
    Let $(\mathbb{X}, \mathcal{X})$ be a measure space and let ${\bf N}(\mathbb{X}) \equiv {\bf N}$ be the family 
    of measures that can be written as a countable sum of finite measures on $\mathbb{X}$ with image in $\mathbb{N}_0$. 
    Let ${\bf \mathcal{N}}(\mathbb{X}) \equiv {\bf \mathcal{N}}$ be the $\sigma$-algebra generated by the collection 
    of subsets $\{\mu \in {\bf N} : \mu(A) = k\}$ over $A \in \mathcal{X}, k \in \mathbb{N}_0$. A {\it point process}
    on $\mathbb{X}$ is a random element $\eta$ of the measure space $({\bf N}, {\bf \mathcal{N}})$
\end{definition}

\begin{definition}[]{Poisson point process}
    Let $\mathbb{X}$ be a space and $\lambda$ an s-finite measure on $\mathbb{X}$. A {\it Poisson point process} 
    with intensity measure $\lambda$ is a point process $\eta$ on $\mathbb{X}$ with
    \begin{enumerate}[(i)]
        \item $\eta(A) \sim \text{Poisson}(\lambda(A))$, that is $\eta(A)$ is Poisson with parameter $\lambda(A)$
        \item If $A_1, \dots, A_n \in \mathbb{X}$ are pairwise disjoint then $\eta(A_1), \dots, \eta(A_n)$ 
        are independent.
    \end{enumerate}
\end{definition}

\newpage

\subsection{Appendix B: Existence Of High Density Packings}

Here I take some notes on chapter 7 of Pach $\&$ Agarwals combinatorial geometry [WILEY GIVES BAD CITATION], 
hoping to find bridges between random geometric graphs and combinatorial geometry. \\ 

As the title suggests, we spend this section looking for the existence of high density packings in $\mathbb{R}^d$. 
We take a non construct approach, instead turning to the probabilistic method. For a convex body $C \subset \mathbb{R}^d$, 
denote the maximal packing (respectively, minimal covering) density by $\delta(C)$ (respectively, $\vartheta(C)$). \\

Let us first consider $C = B^d \subset \mathbb{R}^d$, the $d$-dimensional ball. Then one has the following easy 
observation. 

\begin{proposition}[]{easy lower bound for $\delta(B^d)$}
    Let $B^d \subset \mathbb{R}$ be the $d$-dimensional unit ball. Then, 
    \[\delta(B^d) \geq 2^{-d}\vartheta(B^d) \geq 2^{-d}\]
\end{proposition}

\begin{proof}
    The latter inequality is trivial ($\vartheta(C) \geq 1$). For the former, let us consider a {\it saturated 
    packing} $\mathcal{B} = \{B^d + c_i | i \in \mathbb{N}\}$ (that is, a packing such that 
    no body can be added without intersecting a pre-existing body) and the factor 2 enlargement 
    $\mathcal{B}^\prime = \{2B^d + c_i | i \in \mathbb{N}\}$.
    \begin{claim}
        $\mathcal{B}^\prime$ is a covering of $\mathbb{R}^d$.
    \end{claim}
    \begin{proof}[Proof of claim]
        Suppose $x \notin \cup_{B \in \mathcal{B}^\prime} B$. Then we have $\lvert x - c_i \rvert > 2$ for each $i \geq 1$, 
        which gives \[(B^d + c_i) \cap (B^d + x) = \emptyset, \quad i \geq 1\] Contradiction!
    \end{proof}
    Hence, letting $d(C, \mathbb{R}^d)$ denote the density of a body $C$ in $\mathbb{R}^d$, we clearly have 
    \[\vartheta(B^d) \leq d(\mathcal{B}^\prime, \mathbb{R}^d) = 2^d d(\mathcal{B}, \mathbb{R}^d) \leq 2^d \delta(B^d)\] 
    and the result is proven. 
\end{proof}

In high dimensional Euclidean space, this trivial bound was the best bound known, up to a constant, for a long 
time! In fact, no construction of saturated lattice packings of balls is known (again, in high dimensional space), worth 
checking if this has changed since time of writing. This is surprising, given the obvious drawbacks of this argument. 
If we saturate a packing by continuously adding balls, we can't control the structure of this packing. \\ 

Let $\delta_L(C)$ be the maximal lattice packing density of $C$ into $\mathbb{R}^d$. Using techniques developed by 
Minkowski and Hlawka, we can prove $\delta_L(B^d) > 2^{-d}$. Namely, we can affirm the existence of a lattice packing 
strictly outperforming our previously derived lower bound of $2^{-d}$.  

\begin{definition}[]{star-shaped body}
    Let $C \subset \mathbb{R}^d$ be compact. We call $C$ a {\it star-shaped body} if ${\bf 0}$ is in the interior 
    of $C$ and has the relation ${\bf x} \in C \Rightarrow \lambda {\bf x} \in C$ for all $\lambda \in [0, 1]$.
\end{definition}

\begin{definition}[]{admissible latice}
    Let $\Lambda = \Lambda(u_1, \dots, u_d) = \{m_1 u_1 + \cdots + m_d u_d | m_1, \dots, m_d \in \mathbb{Z}\}$ be a 
    lattice in $\mathbb{R}^d$ and let $C$ be a star-shaped body. Then $\Lambda$ is said to be {\it admissible} for 
    $C$ if it contains no interior point of $C\setminus \{{\bf 0}\}$.
\end{definition}

\begin{definition}[]{critical determinant}
    Let $C$ be a star-shaped body. The {\it critical determinant} of $C$ is defined by 
    \[\Delta(C) := \inf \{\det \Lambda | \Lambda \text{ is admissible for } C\}\]
\end{definition}

\begin{lemma}[]{Mahler selection theorem - 1946}
    Let $\Lambda_1, \Lambda_2, \dots$ be an infinite sequence of lattices in $\mathbb{R}^d$ that have constants 
    $\alpha, \beta > 0$ satisfying, for $i \geq 1$, 
    \begin{enumerate}[(i)]
        \item $\Lambda_i$ is admissible for $\alpha B^d$
        \item $\det \Lambda_i \leq \beta$
    \end{enumerate}
    Then we can select a convergent subsequence $\Lambda_{i_1}, \Lambda_{i_2}, \dots \to \Lambda$.
\end{lemma}

\begin{proof}
    Given as exercise, solve at some point.
\end{proof}

This lemma allows us to prove that the infimum is obtained (verify this also), giving us the following reformulation
of Minkowski's fundamental theorem (or as I prefer, Minkowski's pidgeonhole principle).

\begin{theorem}[]{Minkowski's critical determinant lower bound}
    For any centrally symmetric convex body $C \subset \mathbb{R}^d$, 
    \[\frac{\Delta(C)}{\text{vol}C} \geq \frac{1}{2^d}\]
\end{theorem}

Finding an upper bound for the ratio $\Delta(C)/\text{vol}C$ turns out the be much harder. Minkowski was able to 
give an upper bound, when $C = B^d$, of $\frac{1}{2}(\zeta(d))^{-1}$ where $\zeta(\cdot)$ is the Reimman zeta function.
We prove, thanks to Hlawka, a generalisation of Minkowski's result. 

\begin{lemma}[]{Davenport $\&$ Rogers}
    Let $f:\mathbb{R}^d \to \mathbb{R}$ be a continuous map that vanishes outside of some bounded region. For $\gamma
    \in \mathbb{R}$ set \[V(\gamma) := \int_{\mathbb{R}}\cdots\int_{\mathbb{R}}f(x_1, \dots, x_{d-1}, \gamma)\mathrm{d}x_1 \cdots \mathrm{d}x_{d-1}\] 
    Fix $\delta > 0$ and let $\Lambda^\prime$ be the integer lattice in the hyperplane $x_d = 0$. For a vector $y \in \mathbb{R}^d$ 
    of the form $y = (y_1, \dots, y_{d-1}, \delta)$ define $\Lambda_y$ as the lattice generated by $y$ and $\Lambda^\prime$. 
    Then, \[\int_{0}^1 \cdots \int_{0}^1 \left(\sum_{x \in \Lambda_y, x_d \neq 0}f(x)\right)\mathrm{d}y_1 \cdots \mathrm{d}y_{d-1} 
    = \sum_{i \in Z\setminus \{0\}}V(i \delta)\]
\end{lemma}

\begin{proof}
    Looks like a routine computation but theres one line dependent on voodoo magic, can't wrap my head around it :(
\end{proof}

Now for our generalisation. We work, as suggested, with the probabilistic method. 

\begin{theorem}[]{Hlawka's theorem}
    Let $f: \mathbb{R}^d \to \mathbb{R}$ be a bounded Reimann integrable function that vanishes outside of some bounded
    region and let $\epsilon > 0$. Then there exists a unit lattice $\Lambda$ in $\mathbb{R}^d$ with 
    \[\sum_{{\bf 0} \neq x \in \Lambda}f(x) < \int_{\mathbb{R}^d}f(x)\mathrm{d}x + \epsilon\]
\end{theorem} 

\begin{proof}

\end{proof}

\newpage 

\subsection{Appendix C: A new bound for sphere packing in $\mathbb{R}^d$ as $d \to \infty$}

Notes based off of \cite{campos2023}. Campos utilises a Poisson random geometric graph to improve Hayes' 1947 
bound on the density of the sphere packing problem in high dimensional $\mathbb{R}^d$ by a factor of $\log d$. \\

We denote the maximal density unit-volume sphere packing in $\mathbb{R}^d$ by $\theta(d)$. In this note, we follow 
the work of Campos et al to prove $\theta(d) \to (1 - o(1))\frac{d\log d}{2^{d+1}}$ as $d \to \infty$. It is worth 
noting that this is still an exponential factor behind the best upper bound, which currently is $2^{(-0.599\cdots + o(1))d}$.
An explicit value of $\theta(d)$ is only known for $d \in \{1,2,3,8,24\}$. 

\begin{theorem}[]{$\theta(d)$ bound}
    As $d \to \infty$, one has \[\theta(d) \geq (1 - o(1))\frac{d\log d}{2^{d+1}}\] 
\end{theorem}

Of particular importance is the structural irregularity of the packings used to prove this theorem. Previous work 
relied on lattice packings, whereas this work constructs packings by recursively adding random spheres to $\mathbb{R}^d$
that are centred on points of an appropriately modified Poisson process. \\

Campos $\&$ Co also managed to adapt their techniques to the problem of spherical codes. I will elect to leave that 
to their paper (see thm1.2 \cite{campos2023}).

\subsubsection*{Proof of $\theta(d)$ bound}

Here I describe the method. 
\begin{enumerate}[(i)]
    \item Discretize space with a Poisson process at a carefully chosen intensity.
    \item Impose additional uniformity properties on the discrete point set given in (i) and
    call this space $X \subset \mathbb{R}^d$.
    \item Consider geometric graph $G = G(X, 2r_d)$, $r_d$ the radius of the $d$-dimensional unit-ball.
    \item Bound $\alpha(G)$, the largest independent set in $G$. 
\end{enumerate}

In regards to step (4) we have the following new tool, certainly of independent interest. Let $\Delta(G)$ be the 
maximum vertex degree and let $\Delta_\text{co}(G)$ denote the maximum codegree (that is, the maximal size of 
$|\mathcal{N}(u)\cap \mathcal{N}(v)|$ over vertices $u, v \in V$). 

\begin{theorem}[]{$\alpha(G)$ bound}
    Let $G$ be a graph on $n$ vertices with $\Delta(G) \leq \Delta$ and $\Delta_\text{co}(G) \leq C\Delta (\log \Delta)^{-c}$. 
    Then, \[\alpha(G) \geq (1 - o(1))\frac{n \log \Delta}{\Delta}\] where $o(1) \to 0$ as $\Delta \to \infty$.
\end{theorem}

In fact, one can take $C = 2^{-7}$ and $c = 7$. In fact, this bound is sharp up to some constants $c$ and $C$. To see 
this, $\dots$ [WRITE UP PROOF AT SOME POINT, I GET THE ROUGH IDEA BUT CAN'T FORMALISE].

With this tool, the proof of our $\theta(d)$ bound is remarkably easy. To do so, we just need the following lemma. 

\begin{lemma}[]{large uniform geometric graphs}
    Let $\Omega \subset \mathbb{R}^d$ be bounded and measurable. Then $\forall d \geq 1000 \; \exists X \subset \Omega$ 
    with \[|X| \geq (1-1/d)\frac{\Delta}{2^d}\text{Vol}(\Omega), \quad \text{where} \quad \Delta = 
    \left(\frac{\sqrt{d}}{4\log d}\right)^d\] and, setting $G = G(X, r_d)$ we have 
    \[\Delta(G) \leq \Delta(1 + \Delta^{-1/3}) \quad \text{and} \quad \Delta_\text{co} \leq \Delta e^{-(\log d)^2/8}\]
\end{lemma}

\begin{proof}[Proof of $\theta(d)$ bound]
    Fix $R > 0$ and set $\Omega = B_{{\bf 0}}(R)$. It suffices to prove that we can place \[(1 - o(1))\text{Vol}(\Omega)
    \frac{d\log d}{2^{d+1}}\] points each at pairwise distance bounded below (weakly) by $2r^d$. The previous lemma 
    gives us $X \subset \Omega$ with \[|X| \geq (1-o(1))\text{Vol}(\Omega)\Delta 2^{-d}\] with $G(X, 2r_d)$ satisfying 
    the conditions of our $\alpha(G)$ bound. Applying this gives an independent set $I$ with \[|I| \geq (1-o(1))
    \frac{|X|\log \Delta}{\Delta} \geq (1-o(1))\text{Vol}(\Omega)\frac{d\log d}{2^{d+1}}\] and we're done.
\end{proof}

Proving the the lemma will take some more work. I'll come back to it soon. Interestingly enough I see it uses the 
Mecke equation quite a few times, a lemma seen in Penrose's work. 

\newpage

%%% References
\bibliographystyle{plain}
\bibliography{refs} 

\end{document} 